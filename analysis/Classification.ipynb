{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49da7f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, cophenet\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "######\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "######\n",
    "import uuid\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time    # to be used in loop iterations\n",
    "from datetime import datetime \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pysrc.utility as myutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe939bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'lp': 3,\n",
    "          'const': 'False',\n",
    "          'var': 'False',\n",
    "          'sty': 'False',\n",
    "          'cut': 'False',\n",
    "          'mapID': 4}\n",
    "\n",
    "conf_str = \"-\".join(f\"{k}:{v}\" for k,v in config.items())\n",
    "    \n",
    "# Generazioni variabili per la configurazione\n",
    "lp_l    = range(3,10)\n",
    "const_l = [\"True\",\"False\"]\n",
    "var_l   = [\"True\",\"False\"]\n",
    "sty_l   = [\"True\",\"False\"]\n",
    "cut_l   = [\"True\",\"False\"]\n",
    "mapID_l = range(5)\n",
    "\n",
    "conf_keys = [\"lp\", \"const\", \"var\", \"sty\", \"cut\", \"mapID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7353a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = !ls featuresData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cff870dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    #\"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    #\"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    #\"QDA\",\n",
    "]\n",
    "# svm non lineare, cambiare kernel\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(len(task)),\n",
    "    SVC(C=0.025),\n",
    "    SVC(C=1),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=1000),\n",
    "    RandomForestClassifier(n_estimators=1000),\n",
    "    MLPClassifier(alpha=0.5, max_iter=1000),\n",
    "    #AdaBoostClassifier(n_estimators=1000),\n",
    "    GaussianNB(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc7c6ed2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a1d23233f49fc9805e24cb0779d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No feature in X meets the variance threshold 0.10000 (X contains only one sample)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#th = (dfs[l].var()/dfs[l].mean()).mean()/2# remove everything below the average value of variance/mean\u001b[39;00m\n\u001b[1;32m     58\u001b[0m sel \u001b[38;5;241m=\u001b[39m VarianceThreshold(threshold\u001b[38;5;241m=\u001b[39mth)\n\u001b[0;32m---> 59\u001b[0m df_min \u001b[38;5;241m=\u001b[39m \u001b[43msel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# redo the dataframe\u001b[39;00m\n\u001b[1;32m     61\u001b[0m feats \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m~/_uni/grafe-sim/venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/_uni/grafe-sim/venv/lib/python3.10/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/_uni/grafe-sim/venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/_uni/grafe-sim/venv/lib/python3.10/site-packages/sklearn/feature_selection/_variance_threshold.py:126\u001b[0m, in \u001b[0;36mVarianceThreshold.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    125\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (X contains only one sample)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold))\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: No feature in X meets the variance threshold 0.10000 (X contains only one sample)"
     ]
    }
   ],
   "source": [
    "#config creation\n",
    "conf_list = []\n",
    "for idx, m in enumerate(itertools.product(lp_l, const_l, var_l, sty_l, cut_l, mapID_l)):\n",
    "    conf_keys = [\"lp\", \"const\", \"var\", \"sty\", \"cut\", \"mapID\"]\n",
    "    conf = dict(zip(conf_keys, [m[i] for i in range(len(conf_keys))]))\n",
    "    conf_list.append(conf)\n",
    "\n",
    "# Dataframe result creation\n",
    "df_result = pd.DataFrame()\n",
    "list_of_lines = []    \n",
    "exec_time = {}\n",
    "folding_number = 5\n",
    "\n",
    "timestamp = datetime.now().timestamp()\n",
    "timestamp_str = datetime.fromtimestamp(timestamp).strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "with tqdm(total=len(conf_list)) as pbar:\n",
    "    for idx, config in enumerate(conf_list):\n",
    "        dfs = {}\n",
    "        match = []\n",
    "        conf_str = \"-\".join(f\"{k}:{v}\" for k,v in config.items())\n",
    "        #print(\"conf_str -> \" + conf_str)\n",
    "        \n",
    "        # Find all the folders\n",
    "        lista = !ls featuresData\n",
    "        #print(\"lista -> \" + str(lista))\n",
    "\n",
    "        # Find all the 'csv' in each folder\n",
    "        for i in lista:\n",
    "            match_temp = !ls featuresData/{i} | grep {conf_str}.csv\n",
    "            match.append(i+\"/\"+match_temp[0])\n",
    "            \n",
    "        #print(\"match -> \" + str(match))\n",
    "\n",
    "        task = []\n",
    "        for i, l in enumerate(lista):\n",
    "            task.append(l[1:])\n",
    "\n",
    "        # Aggiunta colonna 'task' e 'id'\n",
    "        for l in match:\n",
    "            p = l.split(\"/\")[0][1:]\n",
    "            dfs[p] = pd.read_csv(\"featuresData/\"+l, sep=\",\")\n",
    "            dfs[p][\"task\"] = int(p)\n",
    "            dfs[p][\"id\"] = dfs[p][\"id\"] + \"-\" + p\n",
    "\n",
    "        # Clean.\n",
    "        for l in task:\n",
    "            #print(f\"Problem: {l}\")\n",
    "            dfs[l] = myutil.remove_zero_rows(dfs[l], exclude_col=[\"task\"])\n",
    "            dfs[l] = myutil.clean_and_index(dfs[l])\n",
    "\n",
    "            #print(\"    Original: \" + str(dfs[l].shape))\n",
    "            # Feature selection (FEATURE CORRELATION) UNFINET FEATURE SELECTION\n",
    "            th = dfs[l].var().mean()\n",
    "                print(th)\n",
    "            #th = (dfs[l].var()/dfs[l].mean()).mean()/2# remove everything below the average value of variance/mean\n",
    "            sel = VarianceThreshold(threshold=th)\n",
    "            df_min = sel.fit_transform(dfs[l])\n",
    "            # redo the dataframe\n",
    "            feats = sel.get_feature_names_out()\n",
    "            index = dfs[l].index\n",
    "            dfs[l] = pd.DataFrame(df_min)\n",
    "            dfs[l] = dfs[l].set_index(index)\n",
    "            dfs[l].columns = feats\n",
    "            #print(\"    After feature selection: \" + str(dfs[l].shape))\n",
    "\n",
    "            dfs[l][\"task\"] = l\n",
    "\n",
    "           # Outlier Elimination (TENERE TUTTO E PROVARE A VEDERE CHE COSA SUCCEDE)\n",
    "            #contamination_level = 0.01 # percentage of contamination\n",
    "            #num_estimators = 300\n",
    "            #iForest = IsolationForest(random_state=0, contamination=contamination_level, n_estimators=num_estimators).fit_predict(dfs[l])\n",
    "            #mask = [i for i, f in enumerate(iForest) if f<0]\n",
    "            #to_drop = dfs[l].iloc[mask].index\n",
    "            #print(to_drop)\n",
    "            #dfs[l] = dfs[l].drop(to_drop)\n",
    "            #print(\"     After outlier elimination: \" + str(dfs[l].shape))\n",
    "\n",
    "        # Concat all df(s).\n",
    "        df_tot = pd.concat([dfs[l.split(\"-\")[0][1:]] for l in lista], ignore_index=False, sort=False)\n",
    "        df_tot = df_tot.fillna(0)\n",
    "        assert(df_tot.shape[0] == sum([dfs[l.split(\"-\")[0][1:]].shape[0] for l in lista]))\n",
    "\n",
    "        df_tot = myutil.drop_unuseful_features_all_equal(df_tot)\n",
    "\n",
    "        print(\"Final shape:\" + str(df_tot.shape))\n",
    "\n",
    "        # Dataset creation\n",
    "        #datasets = []\n",
    "######\n",
    "        X = df_tot.drop('task', axis=1).values\n",
    "        #assert('task' not in X.columns and 'id' not in X.columns)\n",
    "        y = df_tot['task'].values\n",
    "######\n",
    "        #datasets.append([X, y])\n",
    "        \n",
    "        # TO BE MODIFIED, MORE PRECISE\n",
    "        # Confusion matrix e metriche\n",
    "######\n",
    "        for i, clf in enumerate(classifiers):\n",
    "            classifier_time_start = time.time()\n",
    "            \n",
    "            clf = make_pipeline(StandardScaler(), clf)\n",
    "            \n",
    "            actual_classes = np.empty([0], dtype=int)\n",
    "            predicted_classes = np.empty([0], dtype=int)\n",
    "            kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "            \n",
    "            for train_ndx, test_ndx in kfold.split(X):\n",
    "                train_X, train_y, test_X, test_y = X[train_ndx], y[train_ndx], X[test_ndx], y[test_ndx]\n",
    "                \n",
    "                clf.fit(train_X, train_y)\n",
    "                \n",
    "                actual_classes = np.append(actual_classes, test_y)\n",
    "                predicted_classes = np.append(predicted_classes, clf.predict(test_X))\n",
    "                \n",
    "                #score = cross_val_predict(clf, X, y, cv=folding_number, n_jobs=-1)\n",
    "\n",
    "            #score = clf.score(X_test, y_test)\n",
    "            #print(names[i] + \": \" + str(score.mean()) + \" \" + str(score.std()))\n",
    "            \n",
    "            matrix = confusion_matrix(actual_classes, predicted_classes, labels=task)\n",
    "            #print(matrix)\n",
    "            #sns.heatmap(matrix, xticklabels=task, yticklabels=task)\n",
    "            \n",
    "            \n",
    "            classifier_time_end = time.time()\n",
    "            exec_time[f\"compute_{names[i]}\"] = classifier_time_end - classifier_time_start\n",
    "\n",
    "            line = (config | {'class_number': len(lista), 'classifier': names[i], 'matrix_raw': matrix} )\n",
    "            #line = (config | {'class_number': len(lista), 'classifier': names[i], 'score_mean': score.mean(), 'score_std': score.std(), 'score_raw': score} \n",
    "            #        | {'time_clean': exec_time['cleaning'], 'time_down': exec_time['download'], 'time_classifier': exec_time[f\"compute_{names[i]}\"]})\n",
    "            \n",
    "            list_of_lines.append(line)\n",
    "######\n",
    "        #for i, clf in enumerate(classifiees):\n",
    "        #    line = config | dict(zip(names, scores))\n",
    "        #    #print(line)\n",
    "        #    list_of_lines.append(line)\n",
    "\n",
    "        # Save intermidiate results\n",
    "        if (idx+1) % 10 == 0:\n",
    "            df_result_tmp = pd.DataFrame(list_of_lines)\n",
    "            df_result_tmp.to_csv(f\"df_harvesting_{timestamp_str}tmp.csv\")\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "df_result=pd.DataFrame(list_of_lines)\n",
    "df_result.to_csv(f\"df_harvesting_{timestamp_str}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2903d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd24152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ee154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297ff3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
